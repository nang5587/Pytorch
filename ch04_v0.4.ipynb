{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb34616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Malgun Gothic\"\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da73ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, dataframe, target_column=None, transform=None, is_train=True):\n",
    "        self.dataframe = dataframe.copy()\n",
    "        self.target_column = target_column\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self._preprocess()\n",
    "        \n",
    "        if self.is_train and target_column:\n",
    "            self.targets = self.dataframe[target_column].values\n",
    "            self.features = self.dataframe.drop([target_column], axis=1).values\n",
    "        else:\n",
    "            self.targets = None\n",
    "            self.features = self.dataframe.values\n",
    "\n",
    "    def _preprocess(self):\n",
    "        # 불필요한 컬럼을 삭제\n",
    "        columns_to_drop = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n",
    "        existing_columns = [\n",
    "            col for col in columns_to_drop if col in self.dataframe.columns\n",
    "        ]\n",
    "        if existing_columns:\n",
    "            self.dataframe.drop(existing_columns, axis=1, inplace=True)\n",
    "\n",
    "        # 나이 결측값 처리 (중앙값)\n",
    "        if \"Age\" in self.dataframe.columns:\n",
    "            self.dataframe[\"Age\"].fillna(self.dataframe[\"Age\"].median(), inplace=True)\n",
    "\n",
    "        # 승선항구 결측값 처리 (최빈값)\n",
    "        if \"Embarked\" in self.dataframe.columns:\n",
    "            self.dataframe[\"Embarked\"].fillna(\n",
    "                self.dataframe[\"Embarked\"].mode()[0], inplace=True\n",
    "            )\n",
    "\n",
    "        # 요금 (중앙값)\n",
    "        if \"Fare\" in self.dataframe.columns:\n",
    "            self.dataframe[\"Fare\"].fillna(self.dataframe[\"Fare\"].median(), inplace=True)\n",
    "\n",
    "        # 새로운 특성\n",
    "        if \"SibSp\" in self.dataframe.columns and \"Parch\" in self.dataframe.columns:\n",
    "            self.dataframe[\"FamilySize\"] = (\n",
    "                self.dataframe[\"SibSp\"] + self.dataframe[\"Parch\"] + 1\n",
    "            )\n",
    "            self.dataframe[\"IsAlone\"] = (self.dataframe[\"FamilySize\"] == 1).astype(int)\n",
    "\n",
    "        # 나이 그룹\n",
    "        if \"Age\" in self.dataframe.columns:\n",
    "            self.dataframe[\"AgeGroup\"] = pd.cut(\n",
    "                self.dataframe[\"Age\"],\n",
    "                bins=[0, 12, 18, 35, 60, 100],\n",
    "                labels=[0, 1, 2, 3, 4],\n",
    "            ).astype(int)\n",
    "\n",
    "        # 요금 그룹\n",
    "        if \"Fare\" in self.dataframe.columns:\n",
    "            self.dataframe[\"FareGroup\"] = pd.qcut(\n",
    "                self.dataframe[\"Fare\"], q=4, labels=[0, 1, 2, 3]\n",
    "            ).astype(int)\n",
    "\n",
    "        # 원-핫 인코딩\n",
    "        if \"Sex\" in self.dataframe.columns:\n",
    "            sex_dummies = pd.get_dummies(self.dataframe[\"Sex\"], drop_first=True)\n",
    "            self.dataframe = pd.concat([self.dataframe, sex_dummies], axis=1)\n",
    "            self.dataframe.drop([\"Sex\"], axis=1, inplace=True)\n",
    "\n",
    "        if \"Embarked\" in self.dataframe.columns:\n",
    "            embarked_dummies = pd.get_dummies(\n",
    "                self.dataframe[\"Embarked\"], drop_first=True\n",
    "            )\n",
    "            self.dataframe = pd.concat([self.dataframe, embarked_dummies], axis=1)\n",
    "            self.dataframe.drop([\"Embarked\"], axis=1, inplace=True)\n",
    "\n",
    "        # 나머지 결측 (평균)\n",
    "        self.dataframe.fillna(self.dataframe.mean(), inplace=True)\n",
    "        # print(f\"전처리 후 특성 수: {len(self.dataframe.columns)}\")\n",
    "        # print(f\"특성 목록: {list(self.dataframe.columns)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        features = self.features[idx]\n",
    "        \n",
    "        # 변환 적용\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        \n",
    "        features = torch.FloatTensor(features)\n",
    "\n",
    "        if self.is_train and self.targets is not None:\n",
    "            target = torch.LongTensor([self.targets[idx]])[0]\n",
    "            return features, target\n",
    "        else:\n",
    "            return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "180189f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class StandardScaleTransform:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.scaler.fit(data)\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\n",
    "                \"스케일러가 아직 학습되지 않았습니다. fit() 메서드를 먼저 호출하세요.\"\n",
    "            )\n",
    "\n",
    "        if sample.ndim == 1:\n",
    "            sample = sample.reshape(1, -1)\n",
    "            return self.scaler.transform(sample).flatten()\n",
    "        else:\n",
    "            return self.scaler.transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1ab6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "train_data = TitanicDataset(df_train, target_column=\"Survived\")\n",
    "test_data = TitanicDataset(df_test, is_train=False)\n",
    "transform = StandardScaleTransform()\n",
    "transform.fit(train_data.features)\n",
    "train_data.transform = transform\n",
    "test_data.transform = transform\n",
    "train_dataset, val_dataset = random_split(train_data, [0.2, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "414242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ca8c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[256,128,64], dropout_rate=0.3):\n",
    "        super(TitanicNet, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for i, hidden_size in enumerate(hidden_sizes):\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        layers.append(nn.Linear(prev_size, 2))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f788d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    num_epochs=100,\n",
    "    device=\"cpu\",\n",
    "):\n",
    "    \"\"\"모델 훈련 함수\"\"\"\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    patience = 20  # 조기 종료를 위한 patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 학습데이터 가져오는 반복문\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_train += target.size(0)\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "\n",
    "        # 검증 모드\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        # 학습 멈춤\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total_val += target.size(0)\n",
    "                correct_val += (predicted == target).sum().item()\n",
    "\n",
    "        # 평균 계산\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = 100.0 * correct_train / total_train\n",
    "        val_acc = 100.0 * correct_val / total_val\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # 학습률 스케줄러\n",
    "        if scheduler:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # 최고 성능 모델 저장\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(\n",
    "                    f\"=*=*=*= Validation Loss decreased to {avg_val_loss:.6f}. Saving the model! =*=*=*=\"\n",
    "                )\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "                f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "        # 조기 종료\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad7bf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. 모델 훈련 시작...\n",
      "사용 장치: cpu\n",
      "=*=*=*= Validation Loss decreased to 0.444161. Saving the model! =*=*=*=\n",
      "Epoch [20/300] - Train Loss: 0.3046, Train Acc: 86.59%, Val Loss: 0.4830, Val Acc: 81.04%\n",
      "Early stopping at epoch 30\n"
     ]
    }
   ],
   "source": [
    "input_size = train_data.features.shape[1]\n",
    "model = TitanicNet(input_size)\n",
    "\n",
    "# 손실함수와 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=10, factor=0.5\n",
    ")\n",
    "\n",
    "# 5. 모델 훈련\n",
    "print(f\"\\n5. 모델 훈련 시작...\")\n",
    "print(f\"사용 장치: {device}\")\n",
    "\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=300,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
